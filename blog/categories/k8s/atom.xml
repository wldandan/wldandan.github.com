<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: k8s | Happy Coding, Happy Life]]></title>
  <link href="http://wldandan.github.com/blog/categories/k8s/atom.xml" rel="self"/>
  <link href="http://wldandan.github.com/"/>
  <updated>2020-01-26T18:20:21+08:00</updated>
  <id>http://wldandan.github.com/</id>
  <author>
    <name><![CDATA[wldandan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[k8s之Deployment]]></title>
    <link href="http://wldandan.github.com/blog/2019/04/06/k8s-deployment/"/>
    <updated>2019-04-06T14:48:00+08:00</updated>
    <id>http://wldandan.github.com/blog/2019/04/06/k8s-deployment</id>
    <content type="html"><![CDATA[<p>之前我们了解了如何打包，作为Pod中的容器运行，使用临时或者永久存储机制，设置配置项，接下来我们探讨如何部署和升级。</p>

<h5>应用更新的方式</h5>

<p>假定在K8S中存在这样的应用：
* Service
* 3个Pod
* 使用ReplicaSet
* Clients</p>

<p>初始情况，运行V1版本的应用。接下来，我们希望生成V2版本的镜像，并使用V2版本的Pod/容器进行升级。
存在两种方式：
* 先删除V1版本的应用，然后部署V2版本。
* 先新增V2版本的应用，然后删除V1版本。（对于V2版本的新增，可以选择 <code>一次新增全部数量</code>，<code>多次新增，每次部分数量</code>）</p>

<blockquote><p>对于第一种方式：简单，但是存在部署的停机时间<br/>
  对于第二种方式：系统需要同时处理两个版本的应用，尤其是数据Schema需要兼容新旧两个版本</p></blockquote>

<h6>ttt</h6>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[k8s之configmap]]></title>
    <link href="http://wldandan.github.com/blog/2019/04/06/k8s-concept-configmap/"/>
    <updated>2019-04-06T14:48:00+08:00</updated>
    <id>http://wldandan.github.com/blog/2019/04/06/k8s-concept-configmap</id>
    <content type="html"><![CDATA[<p>几乎所有的应用程序都需要配置（实例的配置信息，外部系统的访问信息等），而这些配置显然不应该被打包到应用程序本身中。</p>

<p>本篇文章看看如何在Kubernetes中配置应用程序的信息。</p>

<h4>1.配置容器应用的方式</h4>

<ul>
<li><p>命令行
最简单的应用配置方式，是使用命令行。</p></li>
<li><p>配置文件</p></li>
</ul>


<p>随着配置信息增多，考虑到维护成本，可以将配置信息存储到配置文件中。但对于容器而言，使用配置文件的方式需要将配置项打包到镜像中，而且每次配置信息的变更都会导致重新生成镜像，重新部署，维护和变更成本较高。</p>

<ul>
<li>环境变量</li>
</ul>


<p>在容器应用中，使用环境变量来实现配置，也是较普遍的一种做法，通过将参数传递给容器中的应用，变更容器运行期的配置信息，如MySQL官方的镜像就使用环境变量<code>MYSQL_ROOT_PASSWORD</code> 来修改超级用户root的密码。</p>

<ul>
<li>volume</li>
</ul>


<p>另外，基于volume的方式获取配置信息也是一种可行的方式，如使用Git Repo存储配置信息，能有效的做到版本化管理会随时回退。</p>

<p>在K8S中，存储配置信息的资源被称ConfigMap，本部分将介绍ConfigMap、Secret的使用。</p>

<h4>2.从命令行传递参数</h4>

<p>在Docker容器中，通常使用如下方式传递参数：</p>

<ul>
<li>使用<code>ENTRYPOINT</code>定义可执行命令</li>
<li>使用<code>CMD</code>传递参数</li>
</ul>


<p>在ENTRYPOINT中，可以使用如下两种方式启动应用：</p>

<ul>
<li>Shell方式，如<code>ENTRYPOINT node app.js</code></li>
<li>exec方式，如<code>ENTRYPOINT ["node", "app.js"]</code></li>
</ul>


<blockquote><p>注意: 这两种方式的区别在于前者是先启动Shell，由Shell调用node，而后者直接启动node应用。</p></blockquote>

<p>在K8S中，可以通过配置文件中的<code>command</code>和<code>args</code>来设置容器中的<code>ENTRYPOINT</code>和<code>CMD</code>
譬如</p>

<pre><code class="yaml">kind: Pod
spec:
  containers:
  - image: some/image
    command: ["/bin/command"]
    args: ["arg1", "arg2", "arg3"]
</code></pre>

<p>它们之间的区别如下图所示：</p>

<table>
<thead>
<tr>
<th>Docker</th>
<th>Kubernetes</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>ENTRYPOINT</td>
<td>command</td>
<td>在容器中执行命令</td>
</tr>
<tr>
<td>CMD</td>
<td>args</td>
<td>给命令传递参数</td>
</tr>
</tbody>
</table>


<h4>3.使用环境变量</h4>

<p>在K8S中，使用<code>env</code>设置镜像中定义的环境变量。
譬如，容器中存在如下脚本，其中的<code>INTERVAL</code>使用环境变量进行设置：</p>

<pre><code class="bash">#!/bin/bash
while :
do
  echo $(date)
  sleep $INTERVAL
done
</code></pre>

<p>在K8S中，其配置文件类似如下：
<code>yaml
kind: Pod
spec:
 containers:
 - image: xxxxx
   env:                            
   - name: INTERVAL                
     value: "30"                   
...     
</code></p>

<h4>4.使用CONFIGMAP解耦配置</h4>

<p>Kubernetes允许将配置项分离到一个称为<code>ConfigMap</code>的单独对象中，它包含若干键/值对，并且值的范围可以从文本到整个配置文件。</p>

<p>应用程序不需要直接读取ConfigMap，甚至不需要知道它的存在。Map的内容可以作为环境变量或者卷传递给容器。</p>

<p>使用<code>kubectl</code>创建ConfigMap的过程中，可以指定多样化的配置机制，类似如下所示：</p>

<pre><code class="bash">$ kubectl create configmap my-config
  --from-file=foo.json                     //导入JSON文件
  --from-file=bar=foobar.conf              //导入配置文件   
  --from-file=config-opts/                 //导入目录
  --from-literal=some=thing                //导入文本配置
</code></pre>

<h5>4.1 使用ConfigMap Entry设置环境变量的值</h5>

<p>接下来， 在如上环境变量的例子中，我们使用ConfigMap配置环境变量<code>$INTERVAL</code>的值。</p>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: env-from-configmap
spec:
  containers:
  - image: xxxx
    env:                            
    - name: INTERVAL                 //环境变量名
      valueFrom:                     
        configMapKeyRef:             //使用ConfigMap
          name: fortune-config       //ConfigMap的名称
          key: sleep-interval        //ConfigMap的key
...
</code></pre>

<p>在如上的例子中，读取ConfigMap<code>fortune-config</code>中Key<code>sleep-interval</code>对应的值，作为<code>$INTERVAL</code>的值。</p>

<h5>4.2 使用ConfigMap Entry作为环境变量</h5>

<p>譬如有个ConfigMap，它有两个键，分别是foo、bar。您可以使用envFrom属性将它们全部公开为环境变量，而不是像在前面的示例中那样依次使用env。</p>

<p>如下所示：</p>

<pre><code class="yaml">spec:
  containers:
  - image: some-image
    envFrom:                      //使用envFrom代替Env
    - prefix: CONFIG_             //使用前缀
      configMapRef:               //引用ConfigMap
        name: my-config-map       
...
</code></pre>

<p>CONFIG_作为前缀，将导出如下环境变量CONFIG_foo和CONFIG_bar。当然，前缀是可选的，如不设置，则容器中的环境变量为foo和bar。</p>

<h5>4.3 使用ConfigMap Entry作为命令行参数</h5>

<p>接下来，让我们看看如何将ConfigMap中的值作为参数传递给容器中运行的进程。我们不能在pod.spec.containers.args字段中直接引用ConfigMap，但是可以从ConfigMap中初始化一个环境变量，然后引用参数中的值，相关代码如下所示:</p>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: fortune-args-from-configmap
spec:
  containers:
  - image: xxxxx
    env:                               
    - name: INTERVAL                   
      valueFrom:                       
        configMapKeyRef:               
          name: fortune-config         
          key: sleep-interval          
    args: ["$(INTERVAL)"]              
...
</code></pre>

<p>其关系如图所示：</p>

<p><img src="/images/k8s/k8s-volume-configmap-as-cmd-args.png" /></p>

<h5>4.3 使用ConfigMap volume导出ConfigMap的Entry</h5>

<p>ConfigMap除了可以作为环境变量以及命令行参数外，还可以包括整个目录中的配置文件。</p>

<p>譬如，在<code>configmap-files</code>目录下存在如下2个文件：</p>

<ul>
<li>my-nginx-config.conf</li>
</ul>


<pre><code class="yaml">
server {
  listen              80;
  server_name         www.kubia-example.com;

  gzip on;                                       1
  gzip_types text/plain application/xml;         1

  location / {
    root   /usr/share/nginx/html;
    index  index.html index.htm;
  }
}
</code></pre>

<ul>
<li>sleep-interval.txt
<code>text
25
</code>
接下来，使用命令创建<code>ConfigMap</code>
<code>
$ kubectl create configmap fortune-config --from-file=configmap-files
</code>
然后，我们可以使用volume将ConfigMap中的内容暴露给容器：
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>apiVersion: v1
</span><span class='line'>kind: Pod
</span><span class='line'>metadata:
</span><span class='line'>name: fortune-configmap-volume
</span><span class='line'>spec:
</span><span class='line'>containers:
</span><span class='line'>
</span><span class='line'>&lt;ul&gt;
</span><span class='line'>&lt;li&gt;image: nginx:alpine
</span><span class='line'>name: web-server
</span><span class='line'>volumeMounts:
</span><span class='line'>&hellip;
</span><span class='line'>
</span><span class='line'>&lt;ul&gt;
</span><span class='line'>&lt;li&gt;name: config
</span><span class='line'>mountPath: /etc/nginx/conf.d      //挂载到Pod中的目录
</span><span class='line'>readOnly: true
</span><span class='line'>&hellip;
</span><span class='line'>volumes:
</span><span class='line'>&hellip;&lt;/li&gt;
</span><span class='line'>&lt;/ul&gt;
</span><span class='line'>&lt;/li&gt;
</span><span class='line'>&lt;li&gt;name: config
</span><span class='line'>configMap:                          //使用configMap作为volume的内容
</span><span class='line'>  name: fortune-config            &lt;br/&gt;
</span><span class='line'>&hellip;</span></code></pre></td></tr></table></div></figure></li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K8s之Volume]]></title>
    <link href="http://wldandan.github.com/blog/2019/04/05/k8s-concept-volume/"/>
    <updated>2019-04-05T21:04:00+08:00</updated>
    <id>http://wldandan.github.com/blog/2019/04/05/k8s-concept-volume</id>
    <content type="html"><![CDATA[<p>本篇文章主要介绍在K8S中，pod的容器如何访问外部磁盘存储，以及容器间如何实现共享存储，主要内容包括</p>

<ul>
<li>通过volume和emptyDir在容器中共享数据</li>
<li>在Pod中使用Git Repo</li>
<li>使用外部存储，如GCE</li>
<li>使用PV(PersistentVolume)和PVC(PersistentVolumeClaim)</li>
</ul>


<!--More-->


<h4>一. 使用emptyDir</h4>

<p>最简单的卷类型是emptyDir。顾名思义，emptyDir是从空目录开始。在pod中运行的应用程序可以向它写入任何文件。因为emptyDir卷的生命周期与pod的生命周期相关联，所以当pod被删除时，卷的内容会丢失。</p>

<p>emptyDir卷对于在同一个pod中运行的容器间共享文件特别有用。</p>

<pre><code class="yaml">
apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:
  - image: luksa/fortune                   1
    name: html-generator                   1
    volumeMounts:                          2
    - name: html                           2
      mountPath: /var/htdocs               2
  - image: nginx:alpine                    3
    name: web-server                       3
    volumeMounts:                          4
    - name: html                           4
      mountPath: /usr/share/nginx/html     4
      readOnly: true                       4
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:                                 5
  - name: html                             5
    emptyDir: {}                           5
</code></pre>

<blockquote><ol>
<li>第1个容器名为<code>html-generator</code>，其镜像为<code>luksa/fortune</code></li>
<li>volume的名称是<code>html</code>，其在容器中的路径为<code>/var/htdocs</code></li>
<li>第2个容器为<code>web-server</code>，镜像为<code>nginx:alpine</code></li>
<li>第2个容器使用的卷为<code>html</code>，加载到<code>/usr/share/nginx/html</code></li>
<li>volume名称为<code>name</code>，emptyDir初始为为空</li>
</ol>
</blockquote>

<p>默认情况下，emptyDir使用Node的磁盘，但是你也可以使用内存，类似如下：
<code>yaml
volumes:
  - name: html
    emptyDir:
      medium: Memory              
</code></p>

<h4>二. 使用Git Repo</h4>

<p>Git Repo卷基本上是基于emptyDir，通过克隆Git仓库并在pod启动时（但在创建其容器之前）签出特定版本。如图6.3所示:</p>

<p><img src="/images/k8s/k8s-volume-gitrepo.png" /></p>

<p>示例代码如下所示：
<code>yaml
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    gitRepo:  
      repository: https://github.com/luksa/kubia-website-example.git
      revision: master   
      directory: .       
</code></p>

<p>默认情况下，使用gitrepo卷存在一个缺点，它不会与所引用的git repo保持同步。只有当pod重启或者创建一个新pod时，才会从Git仓库获取最新的内容。</p>

<h4>三. 使用hostPath</h4>

<p>hostPath是一种持久性存储，它指向Node文件系统中的目录，如下图所示。</p>

<p>运行在同一节点上的Pod，如果使用相同的hostPath卷，则可以彼此访问相同的文件。</p>

<p><img src="/images/k8s/k8s-volume-hostpath.png" /></p>

<p>使用hostPath的方式如下所示：</p>

<pre><code class="yaml">
Volumes:
  varlog:
    Type:       HostPath (bare host directory volume)
    Path:       /var/log
  varlibdockercontainers:
    Type:       HostPath (bare host directory volume)
    Path:       /var/lib/docker/containers
</code></pre>

<p>当一个pod被删除时，gitrepo和emptydir卷的内容都会被删除，但hostPath卷的内容不会。对于在某Node上创建的新pod，如果设置hostPath卷与之前pod的路径一致，那么它能也能访问到之前Pod写入的数据。</p>

<p>但是，如果您考虑使用hostPath作为数据库数据目录的话，请谨慎考虑。因为卷的内容存储在特定Node的文件系统中，当数据库pod被重新调度到另一个节点时，数据将无法被看到。</p>

<h4>四. 使用网络文件存储</h4>

<p>对于网络存储而言，可以使用GCE Persistent Disk、AWS Elastic BlockStore、Azure File、Azure Disk。</p>

<p>譬如，使用GCE Persisteng Disk的方式如下所示：</p>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  volumes:
  - name: mongodb-data           1
    gcePersistentDisk:           1
      pdName: mongodb            1
      fsType: ext4               1
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data         2
      mountPath: /data/db        2
    ports:
    - containerPort: 27017
      protocol: TCP
</code></pre>

<blockquote><ol>
<li>使用GCE Persistent Disk.</li>
<li>使用GCE Persistent Disk，挂载到/data/db上</li>
</ol>
</blockquote>

<h4>五. 自建文件系统存储</h4>

<p>除此之外，也可以使用自建的文件系统，如
NFS、iscsi(ISCSI disk)、glusterfs(GlusterFS), rbd(RADOS Block Device), flexVolume, cinder, cephfs, flocker, fc (Fibre Channel)等</p>

<h4>六. 如何将Pod与存储机制解耦</h4>

<p>到目前为止，所有持久卷类型都要求POD的开发人员了解集群实际的网络存储基础结构。例如，要创建一个支持NFS的卷，开发人员必须知道NFS所在的实际服务器。这与Kubernetes的基本理念背道而驰，Kubernetes的目标是对应用程序及其开发人员透明化其背后的基础设施，使他们不必关心基础设施的具体情况，也不必让应用程序在各种云提供商和内部数据中心之间实现数据的移植。</p>

<p>理想情况下，在Kubernetes上部署应用程序的开发人员不必知道底层使用的是哪种存储技术，就像他们不必知道运行其pod所使用的物理服务器类型一样。当开发人员需要为他们的应用程序提供持久化的存储时，他们应该直接能够配置，就像在创建pod时配置CPU、内存和其他资源一样。</p>

<p>为了使应用程序能够在Kubernetes集群中请求存储，而不必处理基础设施的具体细节，K8S引入了两种新的资源:</p>

<ul>
<li>PersistentVolumes</li>
<li>PersistentVolumeClaims</li>
</ul>


<p>使用方式如下所示:</p>

<p><img src="/images/k8s/k8s-volume-pv-and-pvc.png" /></p>

<p>如图所示，开发人员不需要在pod中使用特定的存储机制，集群管理员设置底层存储，然后通过kubernetes创建PV，并且指定其大小和支持的访问模式。</p>

<p>当开发人员需要在pod中使用持久存储时，他们首先创建一个PVC清单，指定所需的大小和访问模式。然后用户将PVC清单提交给kubernetes ，kubernetes找到适当的PV并将其绑定到PV上。</p>

<p>当使用了PV和PVC后，使用GCE Persisent Disk的机制如下所示：</p>

<p><img src="/images/k8s/k8s-volume-pv-vs-volume.png" /></p>

<p>总而言之，在pod中使用持久性存储的最佳方法是创建pvc（必要时使用显式指定的storageclassname），并由动态PersistentVolume Provider负责创建。</p>

<h4>总结</h4>

<p>在K8S中使用存储机制总结如下：</p>

<ul>
<li>使用EmptyDir卷存储临时的非持久性数据</li>
<li>使用gitrepo卷在pod启动时获取git存储库的内容。</li>
<li>使用hostpath卷访问主机节点的文件</li>
<li>在volume中挂在外部存储，在pod启动后保持数据</li>
<li>通过使用persistentvolume和persistentvolumeclaims将pod与存储基础结构分离</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K8s之Service]]></title>
    <link href="http://wldandan.github.com/blog/2019/03/17/k8s-concept-service/"/>
    <updated>2019-03-17T20:45:00+08:00</updated>
    <id>http://wldandan.github.com/blog/2019/03/17/k8s-concept-service</id>
    <content type="html"><![CDATA[<h3>如何访问Pod？</h3>

<p>在非k8s世界中，管理员可以通过在配置文件中指定IP地址或主机名，容许客户端访问，但在k8s中这种方式是行不通的。因为Pod 是有生命周期的，它们可以被创建或销毁。虽然通过 ReplicationController 能够动态地创建Pod，但当Pod被分配到某个节点时，K8s都会为其分配一个IP地址，而该IP地址不总是稳定可依赖的。因此，在 Kubernetes 集群中，如果一组 Pod（称为 backend）为其它 Pod （称为 frontend）提供服务，那么那些 frontend 该如何发现，并连接到这组backend的Pod呢？</p>

<!-- More -->


<h3>Service</h3>

<p>Kubernetes中的Service是一种资源的定义，它将Pod逻辑分组，并提供客户端访问。</p>

<p><img src="/images/k8s/k8s-service-1.png" /></p>

<p> 通过 Label Selector，一组 Pod 能够被暴露为Service，供客户端访问。</p>

<p> <img src="/images/k8s/k8s-service-2.png" /></p>

<p>举个例子，考虑一个图片处理 backend，它运行了3个副本。这些副本是可互换的，通过Service能够解耦frontend与backend的关联。</p>

<blockquote><p>frontend 不需要关心它们调用了哪个 backend 副本。 然而组成这一组 backend 程序的 Pod 实际上可能会发生变化，frontend 客户端不应该也没必要知道，而且也不需要跟踪这一组 backend 的状态。</p></blockquote>

<h3>创建Service</h3>

<p>一个 Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样， Service 定义可以基于 POST 方式，请求 apiserver 创建新的实例。 例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 &ldquo;app=MyApp&rdquo; 标签。</p>

<pre><code class="yaml">kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp            //根据Label选择一组Pod
  ports:
    - protocol: TCP
      port: 80            //用于访问该Service的端口
      targetPort: 9376    //用于访问容器的端口
</code></pre>

<p>接下来，我们可以使用<code>kubectl</code>访问该Service</p>

<p><img src="/images/k8s/k8s-service-3.png" /></p>

<h3>Service如何被外部网络的Client访问</h3>

<p>可以通过如下三种方式，容许外部网络的Client访问Service：</p>

<h4>1.将Service Type置为<code>NodePort</code></h4>

<p>对于NotePort的Service，每个节点(Node)都会打开节点本身的端口，并将该端口上接收到的流量重定向到Service。</p>

<pre><code class="yaml">apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort             //NodePort类型
  ports:
  - port: 80                 //Service内部访问的地址
    targetPort: 8080         //转发给目标Pod的地址
    nodePort: 30123          //可访问的nodeport端口
  selector:
    app: kubia
</code></pre>

<p>使用NodePort的Service如下图所示：
<img src="/images/k8s/k8s-service-4.png" /></p>

<h4>2.将Service Type设置为<code>LoadBalancer</code></h4>

<p>通过设置<code>LoadBalancer</code>，Service可以通过一个专用的负载均衡器来访问（这个均衡器是运行kubernetes的基础设施提供的）。负载均衡器将流量重定向到所有节点上的节点端口。客户机通过负载均衡器的IP连接到服务。</p>

<p>如果kubernetes在不支持LoadBalancer服务的环境中运行，则不会提供负载均衡器，但该服务的行为仍将类似于NodePort服务。</p>

<pre><code class="yaml">
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer                
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
</code></pre>

<p>使用如上配置文件创建Service之后，会调用云基础设施，创建负载均衡器并将其IP地址写入Service对象。一旦结束，IP地址将作为Service的外部IP地址列出：</p>

<p>如下图所示：
<img src="/images/k8s/k8s-service-5.png" /></p>

<h4>3.使用<code>Ingress</code>资源</h4>

<p>这是一种完全不同的机制，通过一个IP地址公开多个服务，它在HTTP（网络层7）上运行，因此可以提供比第4层服务更多的功能。</p>

<pre><code class="yaml">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com               //使用domain name访问service
    http:
      paths:
      - path: /                           //访问的路径
        backend:
          serviceName: kubia-nodeport     
          servicePort: 80                 
</code></pre>

<p>通过Ingress访问Service的流程如下：</p>

<p><img src="/images/k8s/k8s-service-7.png" /></p>

<p>另外，可以通过Ingress访问多个服务：</p>

<pre><code class="yaml">...
  - host: kubia.example.com
    http:
      paths:
      - path: /kubia                
        backend:                    
          serviceName: kubia        
          servicePort: 80           
      - path: /foo                  
        backend:                    
          serviceName: bar          
          servicePort: 80           
</code></pre>

<p><img src="/images/k8s/k8s-service-6.png" /></p>

<h3>Service的诊断</h3>

<p>Service是Kubernetes的关键概念，也是令许多开发人员沮丧的根源。我见过许多开发人员耗费大量时间，来弄清楚为什么无法通过Servic IP或fqdn访问到自己的pods。
出于这个原因，简单介绍一下如何对服务进行故障排除。当无法通过Service访问您的pod时，可以从以下列表开始：</p>

<ul>
<li><p>首先，确保从集群内而不是从外部连接到Service的集群IP。</p></li>
<li><p>不要费心Ping Service的IP来确定服务是否可以访问（记住，服务的集群IP是一个虚拟IP，Ping它永远不会工作）。</p></li>
<li><p>如果您已经定义了一个readiness probe，请确保它是成功的；否则Pod将不属于服务的一部分。</p></li>
<li><p>要确认Pod是服务的一部分，请使用kubectl get endpoints检查相应的endpoint对象。</p></li>
<li><p>如果您试图通过其fqdname或其一部分（例如，myservice.mynamespace.svc.cluster.local或myservice.mynamespace）访问服务，但该服务不起作用，请查看是否可以使用其群集IP而不是fqdname访问该服务。</p></li>
<li><p>检查您是否连接到服务公开的端口，而不是目标端口。</p></li>
<li><p>尝试直接连接到pod ip以确认pod是否接受正确端口上的连接。</p></li>
<li><p>如果你甚至不能通过pod的IP访问你的应用，确保你的应用是否绑定到locahost。</p></li>
</ul>


<h3>总结</h3>

<p>Service是K8s中重要的概念，你应该至少明白Service的这些内容：</p>

<ul>
<li><p>通过Lable selector，将一组Pod设置为Service，并为Service配置静态的IP和端口</p></li>
<li><p>Service可以从Cluster内部访问，也可以通过设置为NodePort或者LoadBalancer的方式从外部访问</p></li>
<li><p>Pod可以通过环境变量获取Service的IP和Port，进行访问</p></li>
<li><p>可以将对Pod的关联关系，设置到Endpoint资源中，而简化label selector的方式</p></li>
<li><p>通过设置ServiceType为<code>ExternalName</code>，可以访问外部的Service</p></li>
<li><p>通过Ingress可以设置多个Service被外部访问</p></li>
<li><p>使用pod的readiness probe可以决定pod是否被作为service的一部分</p></li>
<li><p>通过headless Service，可以使用DNS获取Pod的IP</p></li>
</ul>


<h3>参考</h3>

<p>《Kubernetes in action》
《Kubernetes handbook》</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K8s之控制器]]></title>
    <link href="http://wldandan.github.com/blog/2019/03/16/k8s-concept-controller/"/>
    <updated>2019-03-16T20:45:00+08:00</updated>
    <id>http://wldandan.github.com/blog/2019/03/16/k8s-concept-controller</id>
    <content type="html"><![CDATA[<p>Kubernetes中内建了很多controller（控制器），这些相当于一个状态机，用来控制Pod的具体状态和行为。</p>

<!-- More -->


<p>这里已经讲的很详细了，请<a href="https://jimmysong.io/kubernetes-handbook/concepts/deployment.html">参考</a></p>

<h4>Deployment 是什么？</h4>

<p>Deployment为Pod和Replica Set（下一代Replication Controller）提供声明式更新。</p>

<p>您只需要在 Deployment 中描述期望的目标状态，Deployment controller 会帮您将 Pod 和ReplicaSet 的实际状态改变到您的目标状态。</p>

<p>典型的用例如下：</p>

<ul>
<li>使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。</li>
<li>然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。</li>
<li>如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。</li>
<li>扩容Deployment以满足更高的负载。</li>
<li>暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。</li>
<li>根据Deployment 的状态判断上线是否hang住了。</li>
<li>清除旧的不必要的 ReplicaSet。</li>
</ul>


<h4>创建 Deployment</h4>

<pre><code>$ kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record
deployment "nginx-deployment" created
</code></pre>

<h4>更新Deployment</h4>

<pre><code>$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
deployment "nginx-deployment" image updated
</code></pre>

<h4>检查 Deployment 升级的历史记录</h4>

<pre><code>$ kubectl rollout history deployment/nginx-deployment
deployments "nginx-deployment":
REVISION    CHANGE-CAUSE
1           kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml--record
2           kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
3           kubectl set image deployment/nginx-deployment nginx=nginx:1.91
</code></pre>

<h4>回退到历史版本</h4>

<pre><code>$ kubectl rollout undo deployment/nginx-deployment --to-revision=2
deployment "nginx-deployment" rolled back
</code></pre>

<h4>Deployment 扩容</h4>

<pre><code>$ kubectl scale deployment nginx-deployment --replicas 10
deployment "nginx-deployment" scaled
</code></pre>

<h4>Deployment 状态</h4>

<p>Deployment 在生命周期中有多种状态。在创建一个新的 ReplicaSet 的时候它可以是 <code>progressing</code> 状态， <code>complete</code> 状态，或者 <code>fail to progress</code> 状态。</p>

<h3>编写 Deployment Spec</h3>

<p>在所有的 Kubernetes 配置中，Deployment 也需要<code>apiVersion</code>，<code>kind</code>和<code>metadata</code>这些配置项。配置文件的通用使用说明查看 <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">部署应用</a>，配置容器，和 <a href="https://kubernetes.io/docs/tutorials/object-management-kubectl/object-management/">使用 kubectl 管理资源 </a> 文档。</p>

<h4>Pod Template</h4>

<p> <code>.spec.template</code> 是 <code>.spec</code>中唯一要求的字段。</p>

<p><code>.spec.template</code> 是 <a href="https://kubernetes.io/docs/user-guide/replication-controller/#pod-template">pod template</a>. 它跟 <a href="https://kubernetes.io/docs/user-guide/pods">Pod</a>有一模一样的schema，除了它是嵌套的并且不需要<code>apiVersion</code> 和 <code>kind</code>字段。</p>

<p>另外为了划分Pod的范围，Deployment中的pod template必须指定适当的label（不要跟其他controller重复了，参考<a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment#selector">selector</a>）和适当的重启策略。</p>

<p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle"><code>.spec.template.spec.restartPolicy</code></a> 可以设置为 <code>Always</code> , 如果不指定的话这就是默认配置。</p>

<h4>Replicas</h4>

<p><code>.spec.replicas</code> 是可以选字段，指定期望的pod数量，默认是1。</p>

<h4>Selector</h4>

<p><code>.spec.selector</code>是可选字段，用来指定 <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels">label selector</a> ，圈定Deployment管理的pod范围。</p>

<p>如果被指定， <code>.spec.selector</code> 必须匹配 <code>.spec.template.metadata.labels</code>，否则它将被API拒绝。如果 <code>.spec.selector</code> 没有被指定， <code>.spec.selector.matchLabels</code> 默认是 <code>.spec.template.metadata.labels</code>。</p>

<p>在Pod的template跟<code>.spec.template</code>不同或者数量超过了<code>.spec.replicas</code>规定的数量的情况下，Deployment会杀掉label跟selector不同的Pod。</p>

<p><strong>注意：</strong> 您不应该再创建其他label跟这个selector匹配的pod，或者通过其他Deployment，或者通过其他Controller，例如ReplicaSet和ReplicationController。否则该Deployment会被把它们当成都是自己创建的。Kubernetes不会阻止您这么做。</p>

<p>如果您有多个controller使用了重复的selector，controller们就会互相打架并导致不正确的行为。</p>

<h4>策略</h4>

<p><code>.spec.strategy</code> 指定新的Pod替换旧的Pod的策略。 <code>.spec.strategy.type</code> 可以是"Recreate"或者是 &ldquo;RollingUpdate"。"RollingUpdate"是默认值。</p>

<h5>Recreate Deployment</h5>

<p><code>.spec.strategy.type==Recreate</code>时，在创建出新的Pod之前会先杀掉所有已存在的Pod。</p>

<h5>Rolling Update Deployment</h5>

<p><code>.spec.strategy.type==RollingUpdate</code>时，Deployment使用<a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller">rolling update</a> 的方式更新Pod 。您可以指定<code>maxUnavailable</code> 和 <code>maxSurge</code> 来控制 rolling update 进程。</p>

<h5>Max Unavailable</h5>

<p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> 是可选配置项，用来指定在升级过程中不可用Pod的最大数量。该值可以是一个绝对值（例如5），也可以是期望Pod数量的百分比（例如10%）。通过计算百分比的绝对值向下取整。如果<code>.spec.strategy.rollingUpdate.maxSurge</code> 为0时，这个值不可以为0。默认值是1。</p>

<p>例如，该值设置成30%，启动rolling update后旧的ReplicatSet将会立即缩容到期望的Pod数量的70%。新的Pod ready后，随着新的ReplicaSet的扩容，旧的ReplicaSet会进一步缩容，确保在升级的所有时刻可以用的Pod数量至少是期望Pod数量的70%。</p>

<h5>Max Surge</h5>

<p><code>.spec.strategy.rollingUpdate.maxSurge</code> 是可选配置项，用来指定可以超过期望的Pod数量的最大个数。该值可以是一个绝对值（例如5）或者是期望的Pod数量的百分比（例如10%）。当<code>MaxUnavailable</code>为0时该值不可以为0。通过百分比计算的绝对值向上取整。默认值是1。</p>

<p>例如，该值设置成30%，启动rolling update后新的ReplicatSet将会立即扩容，新老Pod的总数不能超过期望的Pod数量的130%。旧的Pod被杀掉后，新的ReplicaSet将继续扩容，旧的ReplicaSet会进一步缩容，确保在升级的所有时刻所有的Pod数量和不会超过期望Pod数量的130%。</p>

<h4>Progress Deadline Seconds</h4>

<p><code>.spec.progressDeadlineSeconds</code> 是可选配置项，用来指定在系统报告Deployment的<a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment#failed-deployment">failed progressing</a> ——表现为resource的状态中<code>type=Progressing</code>、<code>Status=False</code>、 <code>Reason=ProgressDeadlineExceeded</code>前可以等待的Deployment进行的秒数。Deployment controller会继续重试该Deployment。未来，在实现了自动回滚后， deployment controller在观察到这种状态时就会自动回滚。</p>

<p>如果设置该参数，该值必须大于 <code>.spec.minReadySeconds</code>。</p>

<h4>Min Ready Seconds</h4>

<p><code>.spec.minReadySeconds</code>是一个可选配置项，用来指定没有任何容器crash的Pod并被认为是可用状态的最小秒数。默认是0（Pod在ready后就会被认为是可用状态）。进一步了解什么什么后Pod会被认为是ready状态，参阅 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">Container Probes</a>。</p>

<h4>Rollback To</h4>

<p><code>.spec.rollbackTo</code> 是一个可以选配置项，用来配置Deployment回退的配置。设置该参数将触发回退操作，每次回退完成后，该值就会被清除。</p>

<h4>Revision</h4>

<p><code>.spec.rollbackTo.revision</code>是一个可选配置项，用来指定回退到的revision。默认是0，意味着回退到上一个revision。</p>

<h4>Revision History Limit</h4>

<p>Deployment revision history存储在它控制的ReplicaSets中。</p>

<p><code>.spec.revisionHistoryLimit</code> 是一个可选配置项，用来指定可以保留的旧的ReplicaSet数量。该理想值取决于心Deployment的频率和稳定性。如果该值没有设置的话，默认所有旧的Replicaset或会被保留，将资源存储在etcd中，是用<code>kubectl get rs</code>查看输出。每个Deployment的该配置都保存在ReplicaSet中，然而，一旦您删除的旧的RepelicaSet，您的Deployment就无法再回退到那个revison了。</p>

<p>如果您将该值设置为0，所有具有0个replica的ReplicaSet都会被删除。在这种情况下，新的Deployment rollout无法撤销，因为revision history都被清理掉了。</p>

<h4>Paused</h4>

<p><code>.spec.paused</code>是可以可选配置项，boolean值。用来指定暂停和恢复Deployment。Paused和没有paused的Deployment之间的唯一区别就是，所有对paused deployment中的PodTemplateSpec的修改都不会触发新的rollout。Deployment被创建之后默认是非paused。</p>

<h3>资源参考</h3>

<p><a href="https://jimmysong.io/kubernetes-handbook/concepts/deployment.html">k8s handbook - Deployment</a></p>
]]></content>
  </entry>
  
</feed>
